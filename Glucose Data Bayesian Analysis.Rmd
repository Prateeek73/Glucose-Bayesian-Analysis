---
title: "Glucose Data Bayesian Analysis"
author: "Prateek Singh, Xinyun Liu, Manoj Kumar Surabhi, Xiaolin Liu"
date: "2025-07-30"
output: 
  github_document:
    html_preview: false
    toc: true
---

## Loading libraries

```{r message = FALSE}
set.seed(123)
library(coda)
library(rjags)
```

## Loading up the data

```{r message = FALSE}
#Setting up working directory and loading data
glucose = scan(file="data/glucose.dat")

#Calculating statistics
n <- length(glucose)
ybar <- mean(glucose)
s2 <- var(glucose)
sigma <- sd(glucose)

cat("Calculated Statistics:\n\n",
    "Sample Size (n)        : ", n, "\n",
    "Mean (yÌ„)               : ", round(ybar, 2), "\n",
    "Variance (sÂ²)          : ", round(s2, 2), "\n",
    "Standard Deviation (Ïƒ) : ", round(sigma, 2), "\n")
```

## Visualizing data

```{R}
# Histogram of glucose levels
hist(glucose, breaks=25, probability = TRUE, 
     main="Histogram of Glucose Levels", xlab="Glucose Level")

# Probability density function
lines(density(glucose), col=gray(0.5), lwd=1)

#Normal distribution curve for Î¼ = 121 and Ïƒ = 31, rounded of values
grid.glucose <- seq(min(glucose), max(glucose), length.out = 100)
lines(grid.glucose, dnorm(grid.glucose, round(ybar, 0), round(sigma, 0)), col = "red", lwd = 1, lty = 2)

legend("topright", legend = c("Probability Density Function", 
                              paste("Normal Distribution (Î¼ =", round(ybar, 0), ", Ïƒ =", round(sigma, 0), ")")),
       col = c(gray(0.5), "red"), lty = c(1, 2), lwd = 1, bty='n')
```

#### Skewness

-   The empirical distribution of glucose levels is rightly-skewed.
-   It means the data is more clustered towards lower end with a long tail extending towards right side

#### Kurtosis

-   The empirical distribution exhibits a leptokurtic shape.
-   Leptokurtic indicated sharper peaks and flatter tails of distribution.
-   This suggests a higher concentration of values around the mean and extremes(right tail in out distribution)

##### Outliers

-   There are several outlier values at the higher end of the glucose scale. These contribute to right skewdness and thus increasing the variability

## Gibbs Sampler Helper function

##Mixture normal Distribution

```{R}
gibbs_sampler <- function(y, X, alpha, beta, mu0, tau0, s20, v0, n, S) {
  # Initialize storage for samples
  samples <- list(
    theta1 = numeric(S), theta2 = numeric(S),
    s21 = numeric(S), s22 = numeric(S),
    X = matrix(NA, nrow = S, ncol = n)
  )
  # Initial values for latent variable X and group variances
  s21 <- var(y[X == 1]) 
  s22 <- var(y[X == 2]) 
  for (s in 1:S) {
    # Step 1: Update Ï€ (mixture proportions)
    n1 <- sum(X == 1)
    n2 <- sum(X == 2)
    pi <- rbeta(1, alpha + n1, beta + n2)
    # Step 2: Update Î¸1 and Î¸2 (component means)
    # Calculate the means 
    y1.bar <- mean(y[X == 1])
    y2.bar <- mean(y[X == 2])
    # Updating the precision terms for Î¸1 and Î¸2
    tau1 <- 1 / (1 / tau0 + n1 / s21)
    tau2 <- 1 / (1 / tau0 + n2 / s22)
    # Sample Î¸1 and Î¸2 from normal distributions
    theta1 <- rnorm(1, mean = tau1 * (mu0 / tau0 + n1 * y1.bar / s21), 
                       sd = sqrt(tau1))
    theta2 <- rnorm(1, mean = tau2 * (mu0 / tau0 + n2 * y2.bar / s22), 
                       sd = sqrt(tau2))
    # Step 3: Update Ïƒ1Â² and Ïƒ2Â² (component variances)
    # Calculate the sum of squared deviations
    sum_sq1 <- sum((y[X == 1] - theta1)^2)
    sum_sq2 <- sum((y[X == 2] - theta2)^2)
    # Sample s21 and s22 from Gamma distributions for the variances
    s21 <- 1 / rgamma(1, shape = (v0 + n1) / 2, rate = (v0 * s20 + sum_sq1) / 2)
    s22 <- 1 / rgamma(1, shape = (v0 + n2) / 2, rate = (v0 * s20 + sum_sq2) / 2)
    # Step 4: Update latent variable X
    for (i in 1:n) {
      # Probability from mixtures and sampling bases on normalized probabilities
      prob1 <- pi * dnorm(y[i], mean = theta1, sd = sqrt(s21))
      prob2 <- (1 - pi) * dnorm(y[i], mean = theta2, sd = sqrt(s22))
      probs <- c(prob1, prob2)
      X[i] <- sample(c(1, 2), size = 1, prob = probs / sum(probs))
    }
    # Store samples
    samples$theta1[s] <- theta1
    samples$theta2[s] <- theta2
    samples$s21[s] <- s21
    samples$s22[s] <- s22
    samples$X[s, ] <- X
  }
  return(samples)
}
```

```{R}
# Initial parameter values
alpha <- 1
beta <- 1
mu0 <- 120
tau0 <- 200
s20 <- 1000
v0 <- 10
n <- length(glucose)
S <- 5000
```

## MCMC Diagnostic: Samples splitted(First peak)

### (1) Sample splits (base on median):

### Split the samples to two parts. The first 2,500 samples are used as the first part and the last 2,500 samples are used as the second part. For each part and ðœƒ(1),

##### (a) plot the histogram and kernel density;

##### (b) find the posterior mean and the posterior quantile based 95% confidence interval;

##### (c) plot the trace

##### (d) find the autocorrelation and the effective sample size.

```{R}
results <- gibbs_sampler(y = glucose, X = ifelse(glucose < median(glucose), 1, 2),
                          alpha = alpha, beta = beta,
                          mu0 = mu0, tau0 = tau0, s20 = s20, v0 = v0, n = n, S)
theta1S <- pmin(results$theta1, results$theta2)
theta1S.first = theta1S[1:2500]
theta1S.last = theta1S[2501:5000]

# Part (a): Plot histogram and kernel density
par(mfrow = c(1, 2))
hist(theta1S.first, probability = TRUE,col = "lightblue", border = "black",
     main = "Histogram and KDE (First 2500)", xlab = expression(theta[1]*"(s)"))
lines(density(theta1S.first), col = "red", lwd = 2)
hist(theta1S.last, probability = TRUE, col = "lightblue", border = "black",
     main = "Histogram and KDE (Last 2500)", xlab = expression(theta[1]*"(s)"))
lines(density(theta1S.last), col = "red", lwd = 2)

# Part (b): Compute posterior mean and 95% credible interval
cat("\n[First 2500] Mean   :", round(mean(theta1S.first), 4), 
    "\n[First 2500] 95% CI :", round(quantile(theta1S.first, c(0.025, 0.975)), 4),
    "\n[Last  2500] Mean   :", round(mean(theta1S.last), 4),
    "\n[Last  2500] 95% CI :", round(quantile(theta1S.last, c(0.025, 0.975)), 4))

# Part (c): Plot trace
par(mfrow = c(2, 1), mar = c(2, 2, 3, 0.5))
plot(theta1S.first, type = "l", col = "lightblue", lwd = 2,
     main = "Trace (First 2500 Sample)")
grid()
plot(theta1S.last, type = "l",col = "salmon", lwd = 2, 
     main = "Trace (Last 2500 Samples)")
grid()

# Part (d): Compute autocorrelation and effective sample size
par(mfrow = c(2, 1), mar = c(2, 2, 3, 0.5))
acf_fist <- acf(theta1S.first, lag.max = 100, plot = TRUE, main = "ACF for First 2500 Samples")
acf_last <- acf(theta1S.last, lag.max = 100, plot = TRUE, , main = "ACF for Last 2500 Samples")
cat("\n[First 2500] Autocorrelation - Lag 100 :", round(mean(acf_fist$acf), 4), 
    "\n[First 2500] Effective Sample Size     :", round(effectiveSize(theta1S.first), 0),
    "\n[First 2500] Autocorrelation - Lag 100 :", round(mean(acf_last$acf), 4),
    "\n[First 2500] Effective Sample Size     :", round(effectiveSize(theta1S.last), 0))
```

### Conclusions

-   The histograms and kernel density estimates (KDE) for the first 2500 and last 2500 samples remain nearly identical, indicating that the posterior distribution is stable and consistent across chains.
-   The close values of means and CIs for sample splits are almost same further confirming this
-   The values oscillate around the mean with a consistent variance and are not stuck in a region.
-   The values are not getting stuck in particular region, indicating good mixing,
-   Both ACF and ESS values for the last split is slightly lower than first split but still almost same.

## MCMC Diagnostic (Second peak)

### (2) 2 different chains (based on probability):

### Use the samples from the following initial value of ð‘‹ð‘– - assign ð‘‹ð‘–(0) as 1 with the probability 0.5 and 2 otherwise. Again, the initial value of ðœŽ12 is the sample variance of those ð‘¦ð‘– with ð‘‹ð‘– = 1 and the initial value of ðœŽ12 is the sample variance of those yð‘– with ð‘‹ð‘– = 2. For each chain and ðœƒ(2)

#### (a) plot the histogram and kernel density

#### (b) find the posterior mean and the posterior quantile-based 95% confidence interval

#### (c) plot the trace

#### (d) find the autocorrelation and the effective sample size.

```{R}
chain1.results <- gibbs_sampler(y = glucose, alpha = alpha, beta = beta,
            X = sample(c(1, 2), size = n, replace = TRUE, prob = c(0.5, 0.5)),
            mu0 = mu0, tau0 = tau0, s20 = s20, v0 = v0, n = n,  S = S)
chain2.results <- gibbs_sampler(y = glucose, alpha = alpha, beta = beta,
            X = sample(c(1, 2), size = n, replace = TRUE, prob = c(0.5, 0.5)),
            mu0 = mu0, tau0 = tau0, s20 = s20, v0 = v0, n = n,  S = S)
chain1.theta2S <- pmax(chain1.results$theta1, chain1.results$theta2)
chain2.theta2S <- pmax(chain2.results$theta1, chain2.results$theta2)

# Part (a): Plot histogram and kernel density
par(mfrow = c(1, 2))
hist(chain1.theta2S, probability = TRUE,col = "lightblue", border = "black",
     main = "Histogram and KDE (Chain 1)", xlab = expression(theta[2]*"(s)"))
lines(density(chain1.theta2S), col = "red", lwd = 2)
hist(chain2.theta2S, probability = TRUE, col = "lightblue", border = "black",
     main = "Histogram and KDE (Chain 2)", xlab = expression(theta[2]*"(s)"))
lines(density(chain2.theta2S), col = "red", lwd = 2)

# Part (b): Compute posterior mean and 95% credible interval
cat("\n[Chain 1] Mean   :", round(mean(chain1.theta2S), 4),
    "\n[Chain 1] 95% CI :", round(quantile(chain1.theta2S, c(0.025, 0.975)), 4),
    "\n[Chain 2] Mean   :", round(mean(chain2.theta2S), 4),
    "\n[Chain 2] 95% CI :", round(quantile(chain2.theta2S, c(0.025, 0.975)), 4))

# Part (c): Plot trace
par(mfrow = c(2, 1), mar = c(2, 2, 3, 0.5))
plot(chain1.theta2S, type = "l", col = "lightblue", lwd = 2, main = "Trace (Chain 1)")
grid()
plot(chain2.theta2S, type = "l",col = "salmon", lwd = 2, main = "Trace (Chain 2)")
grid()

# Part (d): Compute autocorrelation and effective sample size
par(mfrow = c(2, 1), mar = c(2, 2, 3, 0.5))
acf_fist <- acf(chain1.theta2S, lag.max = 100, plot = TRUE, main = "ACF for (Chain 1)")
acf_last <- acf(chain2.theta2S, lag.max = 100, plot = TRUE, , main = "ACF for (Chain 2)")
cat("\n[Chain 1] Autocorrelation - Lag 100 :", round(mean(acf_fist$acf), 4),
    "\n[Chain 1] Effective Sample Size     :", round(effectiveSize(chain1.theta2S), 0),
    "\n[Chain 2] Autocorrelation - Lag 100 :", round(mean(acf_last$acf), 4),
    "\n[Chain 2] Effective Sample Size     :", round(effectiveSize(chain2.theta2S), 0))
```

#### Conclusions

-   The histograms and kernel density estimates (KDE) for different chains remain nearly identical, indicating that the posterior distribution is stable and consistent across chains.
-   The close values of means and CIs for sample splits are almost same further confirming this
-   The values oscillate around the mean with a consistent variance and are not stuck in a region.
-   The values are not getting stuck in particular region, indicating good mixing,
-   Both ACF and ESS values for both independent chain are almost same too
-   Grouping Xi randomly with equal 0.5 probability has little to no impact on final results.

**The Gibbs sampler has converged successfully for both split samples and multiple chains. The MCMC diagnostic show us that the posterior distribution is reliable across the sampling process. The consistent posterior means and similar confidence intervals confirms that the results are consistent for both ðœƒ(1) in case of diagnoses with splits and ðœƒ(2) diagnoses with multiple chains**

##### Histogram and KDE:

-   The histograms and KDEs for both halves of the samples are similar, showing a stable and consistent posterior distribution for Î¸, which is approximately normal and centered around the posterior mean.

##### Posterior Mean and Confidence Interval (CI)

-   First 2500 samples: Mean = 103.96, 95% CI = [100.41, 107.68].
-   Last 2500 samples: Mean = 103.93, 95% CI = [100.37, 108.00].
-   The close values in means and CIs indicate that the Gibbs sampler has converged.
-   The mean value of sample is 120 whereas expected value is 103. The reason we have takes min values from min from ðœƒ1 ðœƒ2 samples

##### Trace Plots

-   No visible trend or pattern, suggesting the chains have reached a stationary distribution. The values oscillate around the mean with a consistent variance
-   The chains have reached a stationary distribution with good mixing, suggesting convergence

##### Autocorrelation and Effective Sample Size

-   First 2500 samples: Autocorrelation = 0.1628 , Effective Sample Size = 134.
-   Last 2500 samples: Autocorrelation = 0.1419, Effective Sample Size = 94.
-   The slight increase in autocorrelation and decrease in effective sample size towards the end indicates less precision in the later samples, but the sampler still efficiently explores the parameter space.

**The Gibbs sampler has converged successfully, and the posterior distribution is stable across the sampling process. There is a slight increase in autocorrelation in the later samples where the effective sample size does decrease. However, the consistent posterior means and tight confidence intervals reinforce that the results are reliable.**

## MCMC Diagnostic (Second Peak)

### (3) Multiple chains (Chain split based on probability): Sampliing chain split in two

### Use the samples from the following initial value of ð‘‹ð‘– - assign ð‘‹ð‘–(0) as 1 with the probability 0.5 and 2 otherwise. Again, the initial value of ðœŽ12 is the sample variance of those ð‘¦ð‘– with ð‘‹ð‘– = 1 and the initial value of ðœŽ12 is the sample variance of those yð‘– with ð‘‹ð‘– = 2. For each chain and ðœƒ(2), (a) plot the histogram and kernel density; (b) find the posterior mean and the posterior quantile-based 95% confidence interval; (c) plot the trace; (d) find the autocorrelation and the effective sample size. State your conclusions.

```{R}
results.chains <- list()
for(chain in 1:4){
  cat("\n----------------------------\n")
  cat("MCMC diagnostic for Chain: ", chain)
  cat("\n----------------------------")
  results.chains[[chain]] <- gibbs_sampler(y = glucose, 
                            X = sample(c(1, 2), size = n, replace = TRUE, prob = c(0.5, 0.5)),
                            alpha = alpha, beta = beta,
                            mu0 = mu0, tau0 = tau0, s20 = s20, v0 = v0, n = n,  S = S)
  theta2S <- pmax(results.chains[[chain]]$theta1, results.chains[[chain]]$theta2)
  par(mfrow = c(1, 2))
  hist(theta2S[1:2500], probability = TRUE,col = "lightblue", border = "black",
       main = "Histogram and KDE (Chain 1)", xlab = expression(theta[2]*"(s)"))
  lines(density(theta2S[1:2500]), col = "red", lwd = 2)
  hist(theta2S, probability = TRUE, col = "lightblue", border = "black",
       main = "Histogram and KDE (Chain 2)", xlab = expression(theta[2]*"(s)"))
  lines(density(theta2S), col = "red", lwd = 2)
  
  # Part (b): Compute posterior mean and 95% credible interval
  cat("\n[Chain 1] Mean   :", round(mean(theta2S[1:2500]), 4),
      "\n[Chain 1] 95% CI :", round(quantile(theta2S[1:2500], c(0.025, 0.975)), 4),
      "\n[Chain 2] Mean   :", round(mean(theta2S), 4),
      "\n[Chain 2] 95% CI :", round(quantile(theta2S, c(0.025, 0.975)), 4))
  
  # Part (c): Plot trace
  par(mfrow = c(2, 1), mar = c(2, 2, 3, 0.5))
  plot(theta2S[1:2500], type = "l", col = "lightblue", lwd = 2, main = "Trace (Chain 1)")
  grid()
  plot(theta2S, type = "l",col = "salmon", lwd = 2, main = "Trace (Chain 2)")
  grid()
  
  # Part (d): Compute autocorrelation and effective sample size
  par(mfrow = c(2, 1), mar = c(2, 2, 3, 0.5))
  acf_fist <- acf(theta2S[1:2500], lag.max = 100, plot = TRUE, main = "ACF for (Chain 1)")
  acf_last <- acf(theta2S, lag.max = 100, plot = TRUE, , main = "ACF for (Chain 2)")
  cat("\n[Chain 1] Autocorrelation - Lag 100 :", round(mean(acf_fist$acf), 4),
      "\n[Chain 1] Effective Sample Size     :", round(effectiveSize(chain1.theta2S), 0),
      "\n[Chain 2] Autocorrelation - Lag 100 :", round(mean(acf_last$acf), 4),
      "\n[Chain 2] Effective Sample Size     :", round(effectiveSize(chain2.theta2S), 0))
}
```

#### Conclusions

##### Histogram and KDE:

-   The histograms and kernel density estimates (KDE) for the first and last 2500 samples remain nearly identical, indicating that the posterior distribution is stable and consistent across chains.
-   The distribution is approximately normal, centered around the posterior means, and remains consistent between the initial and final samples, confirming convergence.

##### Posterior Mean and Confidence Interval (CI):

-   First 2500 samples: Mean = 149.88, 95% CI = [137.60, 161.99].
-   Last 2500 samples: Mean = 148.11, 95% CI = [136.35, 161.05].
-   The close values in means and CIs indicate that the Gibbs sampler has converged across multiple chains too which is greater than 120 and justfied we taking max of ðœƒ1 ðœƒ2 values for this diagnostic

##### Trace Plots:

-   The trace plots for both the first and last 2500 samples show no visible trend or pattern, indicating the chains have reached a stationary distribution.
-   The values oscillate around the mean with consistent variance, suggesting good mixing and convergence.

##### Autocorrelation and Effective Sample Size:

-   First 2500 samples: Autocorrelation = 0.2794, Effective Sample Size = 59.
-   Last 2500 samples: Autocorrelation = 0.2901, Effective Sample Size = 52.
-   The slight increase in autocorrelation and decrease in effective sample size in the later samples indicate marginally lower precision, but the sampler effectively explores the parameter space.
-   The effective sample size remaining above 50 suggests sufficient information has been gathered from the posterior distribution.

##### Impact of Xi

-   The random initialization of ð‘‹ð‘–(with 50% probability of starting at 1 or 2) helped improve variability across chains, reducing the likelihood of the sampler getting stuck in local modes.
-   However, the higher autocorrelation and lower effective sample size suggest that the initial variability may not have fully removed some dependence between consecutive samples.

##### Differences

-   The mean of this chains is 137 which is higher than previous mean of 103, suggesting two peaks in our data
-   The ESS values for these chains 59, 52 which is quite less than our previous chain of 134 and 94, suggesting the our data is lot more distributed around first peak as compared to second peak

#### Combined conclusion

-   So, the two-component mixture model fits the glucose data reasonably well, capturing key features of the distribution.
-   The two-component mixture model should adequately capture the data distribution.
-   The Gibbs sampler converges effectively, though with some limitations in ESS and autocorrelation.

## MCMC Diagnostic (JAGS sampling): For each iteration of the Gibbs sampler, sample a value ð‘‹\~2 âˆ’ðµð‘’ð‘Ÿð‘›ð‘œð‘¢ð‘™ð‘™ð‘–(ðœ‹(ð‘ )) and then sample ð‘ŒÌƒ (ð‘ )\~ð‘›ð‘œð‘Ÿð‘šð‘Žð‘™(ðœƒð‘¥(ð‘ ), ðœŽð‘¥(ð‘ )) . Plot a histogram or kernel density estimate for the empirical distribution of {ð‘ŒÌƒ (1), â‹¯ , ð‘ŒÌƒ (ð‘†)}, and compare it to the distribution from the data. Discuss the adequacy of this two-component mixture model for the glucose.

```{R}
# Preparing data and initial values
data_jags <- list(y = glucose, n = length(glucose),
  alpha = alpha, beta = beta, mu0 = mu0, s20 = s20, v0 = v0)
inits <- function(X_init) {
  list(X = X_init,s21 = var(glucose[X_init = 1]), s22 = var(glucose[X_init = 2]))
}
```

### Helper function to find optimal ESS of 1000

```{R}
optimal_samples <- NULL

for (S in seq(5000, 100000, by = 15000)) {

  # Defining and running JAGS model
  jags_model <- jags.model(
  file = "MixtureModel_BetaDistributedPi.R",n.chains = 1, quiet = TRUE,
  data = data_jags, inits = inits(ifelse(glucose < median(glucose), 1, 2)))

  # Sampling
  samples <- coda.samples(
    jags_model, n.iter = S,
    variable.names = c("theta1", "theta2", "s21", "s22", "pi", "y")
  )

  # Calculate effective sample sizes (ESS)
  ess_values <- c(
    theta1 = effectiveSize(as.matrix(samples)[, "theta1"]), 
    theta2 = effectiveSize(as.matrix(samples)[, "theta2"]),
    pi = effectiveSize(as.matrix(samples)[, "pi"]),
    s21 = effectiveSize(as.matrix(samples)[, "s21"]),
    s22 = effectiveSize(as.matrix(samples)[, "s22"])
  )
  
  # Calculating ESS of independent samples from both distributions
  mean_theta <- mean(ess_values[c(1, 2)])
  mean_sigma <- mean(ess_values[c(4, 5)])
  
  # Output ESS values
  cat("ESS values for Sample size =", S, ":\n")
  cat(sprintf("  - ESS for ðœƒ1     : %.0f\n", round(ess_values[1])))
  cat(sprintf("  - ESS for ðœƒ2     : %.0f\n", round(ess_values[2])))
  cat(sprintf("  - ESS for pi     : %.0f\n", round(ess_values[3])))
  cat(sprintf("  - ESS for Ïƒ1     : %.0f\n", round(ess_values[4])))
  cat(sprintf("  - ESS for Ïƒ2     : %.0f\n", round(ess_values[5])))
  cat(sprintf("  - Mean ESS for ðœƒ : %.0f\n", round(mean_theta)))
  cat(sprintf("  - Mean ESS for Ïƒ : %.0f\n", round(mean_sigma)))
  
  if (mean_theta > 1000 && mean_sigma > 1000) {
    cat("Optimal S is ", S, " with mean ESS = ", mean(ess_values))
    optimal_samples <- as.matrix(samples)
    break 
  }
  
}
```

```{R, echo=TRUE}
# Extracting ðœƒð‘¥(ð‘ ), ðœŽð‘¥(ð‘ ) parameters
theta1.samples <- as.matrix(optimal_samples)[, "theta1"]
theta2.samples <- as.matrix(optimal_samples)[, "theta2"]
s21.samples <- as.matrix(optimal_samples)[, "s21"]
s22.samples <- as.matrix(optimal_samples)[, "s22"]
pi.samples <- as.matrix(optimal_samples)[, "pi"]

# Sampling ð‘‹
X_bern <- 2 - rbinom(S, size = 1, prob = pi.samples)

# Sampling ð‘Œ (ð‘ )
Y_samples <- rnorm(S, 
                   mean = ifelse(X_bern == 1, theta1.samples, theta2.samples),
                   sd = ifelse(X_bern == 1, sqrt(s21.samples), sqrt(s22.samples)))

par(mfrow = c(1, 2))
hist(glucose, probability = TRUE, breaks = 20, col = rgb(0.1, 0.2, 0.5, 0.5), border = "black", 
     main = "Histogram of Glucose Data", xlab = "Glucose Values",
     xlim = c(min(glucose), max(glucose)), ylim = c(0, max(density(glucose)$y) * 1.1))
lines(density(glucose), col = "darkred", lwd = 2)
legend("topright", legend = c("Glucose data KDE"), col = c("darkred"), lwd = 2,cex = 0.7)
hist(Y_samples, probability = TRUE, breaks=20, col = rgb(0.1, 0.2, 0.5, 0.5), border = "black",
     main = "Histogram of Y samples", xlab = "Y values", 
     xlim = c(min(glucose), max(glucose)), ylim = c(0, max(density(glucose)$y) * 1.1))
lines(density(Y_samples), col = "darkred", lwd = 2)
legend("topright", legend = c("Y samples KDE"), col = c("darkred"), lwd = 2,cex = 0.7)
```

1.  The KDE of glucose data is skewed with peak shifted towards lower glucose values, with a tail towards higher glucose values and slight second peak towards those higher glucose values

2.  The KDE of sampled Y values from Gibbs sampler follows a similar pattern with slight minor differences.

Both histograms and KDE have a similar shape and distribution. So the two-component model is reasonably good to capture overall shape and spread of our glucose data distribution. Slight differences in distribution's peak and tail can be tuned by a more complex model like hierarchical model .

## MCMC Diagnsotic (For Effective sample size 1000)

```{R}
theta1S <- pmin(optimal_samples[, "theta1"], optimal_samples[, "theta2"])
theta2S <- pmax(optimal_samples[, "theta1"], optimal_samples[, "theta2"])

# Part (a): Plot histogram and kernel density
par(mfrow = c(1, 2))
hist(theta1S, probability = TRUE,col = "lightblue", border = "black",
     main = "Histogram and KDE", xlab = expression(theta[1]*"(s)"))
lines(density(theta1S), col = "red", lwd = 2)
hist(theta2S, probability = TRUE, col = "lightblue", border = "black",
     main = "Histogram and KDE", xlab = expression(theta[2]*"(s)"))
lines(density(theta2S), col = "red", lwd = 2)
# Part (b): Compute posterior mean and 95% credible interval
cat("\n[Theta 1] Mean   :", round(mean(theta1S), 4),
    "\n[Theta 1] 95% CI :", round(quantile(theta1S, c(0.025, 0.975)), 4),
    "\n[Theta 2] Mean   :", round(mean(theta2S), 4),
    "\n[Theta 2] 95% CI :", round(quantile(theta2S, c(0.025, 0.975)), 4))
# Part (c): Plot trace
par(mfrow = c(2, 1), mar = c(2, 2, 3, 0.5))
plot(theta1S, type = "l", col = "lightblue", lwd = 2, 
     main = expression("Theta "*theta[1]*"(s)"))
grid()
plot(theta2S, type = "l",col = "salmon", lwd = 2, 
     main = expression("Theta "*theta[2]*"(s)"))
grid()
# Part (d): Compute autocorrelation and effective sample size
par(mfrow = c(2, 1), mar = c(2, 2, 3, 0.5))
acf_theta1 <- acf(theta1S, lag.max = 100,
                  plot = TRUE, main = expression("ACF of "*theta[1]*"(s)"))
acf_theta2 <- acf(theta2S, lag.max = 100,
                  plot = TRUE, main = expression("ACF of "*theta[2]*"(s)"))
cat("\n[Theta 1] Autocorrelation - Lag 100 :", round(mean(acf_theta1$acf), 4),
    "\n[Theta 1] Effective Sample Size     :", round(effectiveSize(theta1S), 0),
    "\n[Theta 2] Autocorrelation - Lag 100 :", round(mean(acf_theta2$acf), 4),
    "\n[Theta 2] Effective Sample Size     :", round(effectiveSize(theta2S), 0))
```
